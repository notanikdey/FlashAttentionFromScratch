{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPYGO1kW2w6bQX1xSrpvrC7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "DEVICE = triton.runtime.driver.active.get_active_torch_device()"
      ],
      "metadata": {
        "id": "JxshzW1Pej2_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def add_kernel(x_ptr, #ptr to first element of x input vec\n",
        "               y_ptr, #ptr to first element of y input vec\n",
        "               output_ptr, #ptr to output vec\n",
        "               n_elements, #size of the vec\n",
        "               BLOCK_SIZE: tl.constexpr\n",
        "               #no of elems each program(block) should process and\n",
        "               #we use 'constexpr' so it can be used as a shape value\n",
        "               ):\n",
        "  #there are multiple programs (block) processing different data\n",
        "  #and we have to identify which program\n",
        "  pid = tl.program_id(axis=0) #we use a 1D launch grid so axis is 0\n",
        "  #this program will process inputs that are offset from the initial data\n",
        "  #for instance, if you had a vector of length 256 and block_size of 64, the programs\n",
        "  #would each access the elements [0:64, 64:128, 128:192, 192:256]\n",
        "  #offsets is a list of pointers\n",
        "  block_start = pid*BLOCK_SIZE\n",
        "  offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "  #create a mask to not access out of bounds elements i.e. elements that don't exist\n",
        "  #since n_elements might not be a multiple of block_size, the last block will be created\n",
        "  #with only some of the threads required so we have to turn the rest of threads in the block off\n",
        "  mask = offsets < n_elements\n",
        "  #we now load x and y from DRAM, masking out any extra elems in case the input\n",
        "  #is not a multiple of block size\n",
        "  x = tl.load(x_ptr + offsets, mask=mask)\n",
        "  y = tl.load(y_ptr + offsets, mask=mask)\n",
        "  output = x + y\n",
        "  #write x + y i.e. output back to DRAM\n",
        "  tl.store(output_ptr + offsets, output, mask=mask)\n"
      ],
      "metadata": {
        "id": "6g5IWanZvcRF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a helper function to allocate the output tensor and enqueue the above kernel with appropriate grid/block sizes\n",
        "\n",
        "def add(x: torch.Tensor, y: torch.Tensor):\n",
        "  #we need to preallocate the output\n",
        "  output = torch.empty_like(x)\n",
        "  assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE\n",
        "  n_elements = output.numel()\n",
        "  #SPMD launch grid denotes the number of kernel instances that run in parallel\n",
        "  #in this case, we use a 1D launch grid where the size is the number of blocks\n",
        "  grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
        "\n",
        "  add_kernel[grid](x,y, output, n_elements, BLOCK_SIZE=1024)\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "2WVNP7b00id7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "EgVBjmE_3Qkm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "size = 100000000\n",
        "x = torch.rand(size, device=DEVICE)\n",
        "y = torch.rand(size, device=DEVICE)\n",
        "\n",
        "start_torch = time.time()\n",
        "output_torch = x + y\n",
        "end_torch = time.time()\n",
        "\n",
        "start_ton = time.time()\n",
        "output_triton = add(x,y)\n",
        "end_ton = time.time()\n",
        "\n",
        "print(output_torch)\n",
        "print(output_triton)\n",
        "print(f'The maximum difference between torch and triton is '\n",
        "      f'{torch.max(torch.abs(output_torch - output_triton))}')\n",
        "\n",
        "print(f\"Time taken by torch is: {end_torch-start_torch} seconds\")\n",
        "print(f\"Time taken by triton is: {end_ton-start_ton} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_HG3SCN23JR",
        "outputId": "25d4171d-37e2-4116-ef97-48289faec13a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9809, 0.6194, 0.9237,  ..., 0.8593, 1.6668, 1.0278], device='cuda:0')\n",
            "tensor([0.9809, 0.6194, 0.9237,  ..., 0.8593, 1.6668, 1.0278], device='cuda:0')\n",
            "The maximum difference between torch and triton is 0.0\n",
            "Time taken by torch is: 0.0005977153778076172 seconds\n",
            "Time taken by triton is: 0.0005478858947753906 seconds\n"
          ]
        }
      ]
    }
  ]
}