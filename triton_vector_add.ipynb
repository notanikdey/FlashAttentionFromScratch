{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUmXTEFSzlSOaf2ZNdYXlX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "DEVICE = triton.runtime.driver.active.get_active_torch_device()"
      ],
      "metadata": {
        "id": "JxshzW1Pej2_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def add_kernel(x_ptr, #ptr to first element of x input vec\n",
        "               y_ptr, #ptr to first element of y input vec\n",
        "               output_ptr, #ptr to output vec\n",
        "               n_elements, #size of the vec\n",
        "               BLOCK_SIZE: tl.constexpr\n",
        "               #no of elems each program(block) should process and\n",
        "               #we use 'constexpr' so it can be used as a shape value\n",
        "               ):\n",
        "  #there are multiple programs (block) processing different data\n",
        "  #and we have to identify which program\n",
        "  pid = tl.program_id(axis=0) #we use a 1D launch grid so axis is 0\n",
        "  #this program will process inputs that are offset from the initial data\n",
        "  #for instance, if you had a vector of length 256 and block_size of 64, the programs\n",
        "  #would each access the elements [0:64, 64:128, 128:192, 192:256]\n",
        "  #offsets is a list of pointers\n",
        "  block_start = pid*BLOCK_SIZE\n",
        "  offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "  #create a mask to not access out of bounds elements i.e. elements that don't exist\n",
        "  #since n_elements might not be a multiple of block_size, the last block will be created\n",
        "  #with only some of the threads required so we have to turn the rest of threads in the block off\n",
        "  mask = offsets < n_elements\n",
        "  #we now load x and y from DRAM, masking out any extra elems in case the input\n",
        "  #is not a multiple of block size\n",
        "  x = tl.load(x_ptr + offsets, mask=mask)\n",
        "  y = tl.load(y_ptr + offsets, mask=mask)\n",
        "  output = x + y\n",
        "  #write x + y i.e. output back to DRAM\n",
        "  tl.store(output_ptr + offsets, output, mask=mask)\n"
      ],
      "metadata": {
        "id": "6g5IWanZvcRF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}